{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 19:53:42.702106 4745409984 secure_random.py:22] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.14.0). Fix this by compiling custom ops.\n",
      "W0829 19:53:42.855949 4745409984 deprecation_wrapper.py:119] From /Users/lek/miniconda3/envs/pysyft/lib/python3.6/site-packages/tf_encrypted/session.py:28: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import syft as sy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "        self.verbose = True\n",
    "        self.cuda = True\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)\n",
    "\n",
    "use_cuda = args.cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 20:09:42.982542 4745409984 hook.py:102] Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VirtualWorker id:alice #objects:0>, <VirtualWorker id:bob #objects:0>]\n",
      "<VirtualWorker id:charlie #objects:0>\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "alice = sy.VirtualWorker(hook, id='alice')\n",
    "bob = sy.VirtualWorker(hook, id='bob')\n",
    "charlie = sy.VirtualWorker(hook, id='charlie')\n",
    "\n",
    "workers = [alice, bob]\n",
    "crypto_provider = charlie\n",
    "print(workers)\n",
    "print(crypto_provider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 640\n",
    "n_test_items = 640\n",
    "\n",
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../data/F_MNIST_data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../data/F_MNIST_data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/640 (0%)]\tLoss: 1.072000\tTime: 26.362s\n",
      "Train Epoch: 1 [64/640 (10%)]\tLoss: 1.072000\tTime: 26.524s\n",
      "Train Epoch: 1 [128/640 (20%)]\tLoss: 1.011000\tTime: 26.963s\n",
      "Train Epoch: 1 [192/640 (30%)]\tLoss: 0.963000\tTime: 26.966s\n",
      "Train Epoch: 1 [256/640 (40%)]\tLoss: 0.974000\tTime: 26.526s\n",
      "Train Epoch: 1 [320/640 (50%)]\tLoss: 0.943000\tTime: 26.630s\n",
      "Train Epoch: 1 [384/640 (60%)]\tLoss: 0.923000\tTime: 26.716s\n",
      "Train Epoch: 1 [448/640 (70%)]\tLoss: 0.921000\tTime: 26.751s\n",
      "Train Epoch: 1 [512/640 (80%)]\tLoss: 0.924000\tTime: 26.632s\n",
      "Train Epoch: 1 [576/640 (90%)]\tLoss: 0.892000\tTime: 26.775s\n",
      "\n",
      "Test set: Accuracy: 95.0/640 (15%)\n",
      "\n",
      "Train Epoch: 2 [0/640 (0%)]\tLoss: 0.884000\tTime: 26.682s\n",
      "Train Epoch: 2 [64/640 (10%)]\tLoss: 0.887000\tTime: 26.626s\n",
      "Train Epoch: 2 [128/640 (20%)]\tLoss: 0.867000\tTime: 26.564s\n",
      "Train Epoch: 2 [192/640 (30%)]\tLoss: 0.865000\tTime: 26.583s\n",
      "Train Epoch: 2 [256/640 (40%)]\tLoss: 0.865000\tTime: 26.586s\n",
      "Train Epoch: 2 [320/640 (50%)]\tLoss: 0.853000\tTime: 27.541s\n",
      "Train Epoch: 2 [384/640 (60%)]\tLoss: 0.840000\tTime: 27.126s\n",
      "Train Epoch: 2 [448/640 (70%)]\tLoss: 0.853000\tTime: 26.805s\n",
      "Train Epoch: 2 [512/640 (80%)]\tLoss: 0.846000\tTime: 26.819s\n",
      "Train Epoch: 2 [576/640 (90%)]\tLoss: 0.829000\tTime: 26.825s\n",
      "\n",
      "Test set: Accuracy: 246.0/640 (38%)\n",
      "\n",
      "Train Epoch: 3 [0/640 (0%)]\tLoss: 0.834000\tTime: 26.961s\n",
      "Train Epoch: 3 [64/640 (10%)]\tLoss: 0.825000\tTime: 27.965s\n",
      "Train Epoch: 3 [128/640 (20%)]\tLoss: 0.814000\tTime: 28.187s\n",
      "Train Epoch: 3 [192/640 (30%)]\tLoss: 0.830000\tTime: 28.381s\n",
      "Train Epoch: 3 [256/640 (40%)]\tLoss: 0.822000\tTime: 28.022s\n",
      "Train Epoch: 3 [320/640 (50%)]\tLoss: 0.813000\tTime: 28.111s\n",
      "Train Epoch: 3 [384/640 (60%)]\tLoss: 0.802000\tTime: 28.301s\n",
      "Train Epoch: 3 [448/640 (70%)]\tLoss: 0.816000\tTime: 29.575s\n",
      "Train Epoch: 3 [512/640 (80%)]\tLoss: 0.802000\tTime: 29.741s\n",
      "Train Epoch: 3 [576/640 (90%)]\tLoss: 0.792000\tTime: 27.774s\n",
      "\n",
      "Test set: Accuracy: 311.0/640 (49%)\n",
      "\n",
      "Train Epoch: 4 [0/640 (0%)]\tLoss: 0.802000\tTime: 28.779s\n",
      "Train Epoch: 4 [64/640 (10%)]\tLoss: 0.779000\tTime: 29.706s\n",
      "Train Epoch: 4 [128/640 (20%)]\tLoss: 0.778000\tTime: 30.341s\n",
      "Train Epoch: 4 [192/640 (30%)]\tLoss: 0.805000\tTime: 28.978s\n",
      "Train Epoch: 4 [256/640 (40%)]\tLoss: 0.787000\tTime: 28.657s\n",
      "Train Epoch: 4 [320/640 (50%)]\tLoss: 0.782000\tTime: 28.577s\n",
      "Train Epoch: 4 [384/640 (60%)]\tLoss: 0.769000\tTime: 28.788s\n",
      "Train Epoch: 4 [448/640 (70%)]\tLoss: 0.782000\tTime: 28.666s\n",
      "Train Epoch: 4 [512/640 (80%)]\tLoss: 0.763000\tTime: 28.600s\n",
      "Train Epoch: 4 [576/640 (90%)]\tLoss: 0.756000\tTime: 29.088s\n",
      "\n",
      "Test set: Accuracy: 326.0/640 (51%)\n",
      "\n",
      "Train Epoch: 5 [0/640 (0%)]\tLoss: 0.772000\tTime: 28.647s\n",
      "Train Epoch: 5 [64/640 (10%)]\tLoss: 0.738000\tTime: 28.747s\n",
      "Train Epoch: 5 [128/640 (20%)]\tLoss: 0.744000\tTime: 28.673s\n",
      "Train Epoch: 5 [192/640 (30%)]\tLoss: 0.778000\tTime: 29.191s\n",
      "Train Epoch: 5 [256/640 (40%)]\tLoss: 0.753000\tTime: 28.993s\n",
      "Train Epoch: 5 [320/640 (50%)]\tLoss: 0.748000\tTime: 28.879s\n",
      "Train Epoch: 5 [384/640 (60%)]\tLoss: 0.733000\tTime: 30.750s\n",
      "Train Epoch: 5 [448/640 (70%)]\tLoss: 0.747000\tTime: 29.213s\n",
      "Train Epoch: 5 [512/640 (80%)]\tLoss: 0.724000\tTime: 30.827s\n",
      "Train Epoch: 5 [576/640 (90%)]\tLoss: 0.721000\tTime: 28.675s\n",
      "\n",
      "Test set: Accuracy: 343.0/640 (54%)\n",
      "\n",
      "Train Epoch: 6 [0/640 (0%)]\tLoss: 0.742000\tTime: 28.532s\n",
      "Train Epoch: 6 [64/640 (10%)]\tLoss: 0.696000\tTime: 28.700s\n",
      "Train Epoch: 6 [128/640 (20%)]\tLoss: 0.709000\tTime: 29.461s\n",
      "Train Epoch: 6 [192/640 (30%)]\tLoss: 0.751000\tTime: 28.701s\n",
      "Train Epoch: 6 [256/640 (40%)]\tLoss: 0.722000\tTime: 28.667s\n",
      "Train Epoch: 6 [320/640 (50%)]\tLoss: 0.719000\tTime: 28.567s\n",
      "Train Epoch: 6 [384/640 (60%)]\tLoss: 0.700000\tTime: 28.593s\n",
      "Train Epoch: 6 [448/640 (70%)]\tLoss: 0.717000\tTime: 28.618s\n",
      "Train Epoch: 6 [512/640 (80%)]\tLoss: 0.688000\tTime: 28.529s\n",
      "Train Epoch: 6 [576/640 (90%)]\tLoss: 0.688000\tTime: 28.565s\n",
      "\n",
      "Test set: Accuracy: 368.0/640 (58%)\n",
      "\n",
      "Train Epoch: 7 [0/640 (0%)]\tLoss: 0.713000\tTime: 28.782s\n",
      "Train Epoch: 7 [64/640 (10%)]\tLoss: 0.662000\tTime: 28.777s\n",
      "Train Epoch: 7 [128/640 (20%)]\tLoss: 0.678000\tTime: 28.765s\n",
      "Train Epoch: 7 [192/640 (30%)]\tLoss: 0.727000\tTime: 28.988s\n",
      "Train Epoch: 7 [256/640 (40%)]\tLoss: 0.696000\tTime: 28.842s\n",
      "Train Epoch: 7 [320/640 (50%)]\tLoss: 0.689000\tTime: 29.030s\n",
      "Train Epoch: 7 [384/640 (60%)]\tLoss: 0.667000\tTime: 29.626s\n",
      "Train Epoch: 7 [448/640 (70%)]\tLoss: 0.692000\tTime: 29.565s\n",
      "Train Epoch: 7 [512/640 (80%)]\tLoss: 0.657000\tTime: 29.148s\n",
      "Train Epoch: 7 [576/640 (90%)]\tLoss: 0.657000\tTime: 28.899s\n",
      "\n",
      "Test set: Accuracy: 382.0/640 (60%)\n",
      "\n",
      "Train Epoch: 8 [0/640 (0%)]\tLoss: 0.691000\tTime: 28.579s\n",
      "Train Epoch: 8 [64/640 (10%)]\tLoss: 0.626000\tTime: 28.814s\n",
      "Train Epoch: 8 [128/640 (20%)]\tLoss: 0.645000\tTime: 28.786s\n",
      "Train Epoch: 8 [192/640 (30%)]\tLoss: 0.706000\tTime: 28.800s\n",
      "Train Epoch: 8 [256/640 (40%)]\tLoss: 0.671000\tTime: 29.752s\n",
      "Train Epoch: 8 [320/640 (50%)]\tLoss: 0.662000\tTime: 28.849s\n",
      "Train Epoch: 8 [384/640 (60%)]\tLoss: 0.635000\tTime: 28.765s\n",
      "Train Epoch: 8 [448/640 (70%)]\tLoss: 0.664000\tTime: 28.929s\n",
      "Train Epoch: 8 [512/640 (80%)]\tLoss: 0.628000\tTime: 28.891s\n",
      "Train Epoch: 8 [576/640 (90%)]\tLoss: 0.629000\tTime: 28.984s\n",
      "\n",
      "Test set: Accuracy: 394.0/640 (62%)\n",
      "\n",
      "Train Epoch: 9 [0/640 (0%)]\tLoss: 0.663000\tTime: 28.840s\n",
      "Train Epoch: 9 [64/640 (10%)]\tLoss: 0.596000\tTime: 28.785s\n",
      "Train Epoch: 9 [128/640 (20%)]\tLoss: 0.618000\tTime: 28.859s\n",
      "Train Epoch: 9 [192/640 (30%)]\tLoss: 0.684000\tTime: 28.847s\n",
      "Train Epoch: 9 [256/640 (40%)]\tLoss: 0.647000\tTime: 29.785s\n",
      "Train Epoch: 9 [320/640 (50%)]\tLoss: 0.637000\tTime: 30.266s\n",
      "Train Epoch: 9 [384/640 (60%)]\tLoss: 0.605000\tTime: 29.051s\n",
      "Train Epoch: 9 [448/640 (70%)]\tLoss: 0.638000\tTime: 29.200s\n",
      "Train Epoch: 9 [512/640 (80%)]\tLoss: 0.599000\tTime: 29.197s\n",
      "Train Epoch: 9 [576/640 (90%)]\tLoss: 0.605000\tTime: 29.091s\n",
      "\n",
      "Test set: Accuracy: 404.0/640 (63%)\n",
      "\n",
      "Train Epoch: 10 [0/640 (0%)]\tLoss: 0.639000\tTime: 29.054s\n",
      "Train Epoch: 10 [64/640 (10%)]\tLoss: 0.565000\tTime: 29.140s\n",
      "Train Epoch: 10 [128/640 (20%)]\tLoss: 0.590000\tTime: 28.523s\n",
      "Train Epoch: 10 [192/640 (30%)]\tLoss: 0.667000\tTime: 27.627s\n",
      "Train Epoch: 10 [256/640 (40%)]\tLoss: 0.624000\tTime: 28.562s\n",
      "Train Epoch: 10 [320/640 (50%)]\tLoss: 0.613000\tTime: 27.378s\n",
      "Train Epoch: 10 [384/640 (60%)]\tLoss: 0.577000\tTime: 27.673s\n",
      "Train Epoch: 10 [448/640 (70%)]\tLoss: 0.612000\tTime: 27.282s\n",
      "Train Epoch: 10 [512/640 (80%)]\tLoss: 0.573000\tTime: 26.438s\n",
      "Train Epoch: 10 [576/640 (90%)]\tLoss: 0.582000\tTime: 26.577s\n",
      "\n",
      "Test set: Accuracy: 408.0/640 (64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
